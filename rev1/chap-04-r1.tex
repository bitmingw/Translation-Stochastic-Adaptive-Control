\chapter{可控马尔可夫链模型} % Controlled Markov Chain Models

在本章将介绍有限可控马尔可夫模型下随机系统的一些基本结论。在观测和反馈法则只取决于反馈回路的情况下，状态过程被称为马尔可夫过程。本章将复习马尔可夫链的渐进性质，并简要介绍无限状态马尔可夫链。最后，本章将提出一个在某些情形下把一个连续时间马尔可夫过程转化为离散时间马尔可夫过程的方法。我们通过一个排序系统讲解此方法。

\section{一个例子}

迄今为止状态变量的取值范围均为$R^n$，但在很多情况下，让状态只能取一个有限集合的数值是更有利的。下面通过一个例子来说明。

有这样一台机器，在时刻$k$的状态由$x_k$来描述，$x_k$根据机器是否正常运转会返回1或者2。现在假定不允许从外界对机器施加控制，机器处于自治状态。若在$k$时刻机器正常运行，则$x_k = 1$，且它有一定的概率$q > 0$会在下一时刻出错，也就是$x_{k+1} = 2$,同时，有$1 - q$的概率它会维持正常运行的状态，即$x_{k+1} = 1$。若假设$q$与先前的状态$x_{k-1},...x_0$无关，并且出错的机器会一直保持错误的状态（当$x_k = 2$时$x_{k+1} = 2$的概率为1），则称$\{x_k, k \ge 0\}$是一个马尔可夫链。它的转移概率由矩阵$P = P\{P_ij\}$描述：
\begin{equation}
\begin{bmatrix}
1-q & q \\
0 & 1
\end{bmatrix}. \label{eq:4.1.1}
\end{equation}
转移概率矩阵的所有元素均为非负值，且每一行各元素之和为1，这样的矩阵又称为随机矩阵。

马尔可夫性质由下式表示：
\begin{equation}
Prob\{x_{k+1} = j \vert x_k = i, x_{k-1},...,x_0\} = P_ij, \quad i,j \in \{1,2\}. \label{eq:4.1.2}
\end{equation}

% ==== End of Page 35 ====

下面我们引入两个控制动作。设$u_k^1$是$k$时刻机器工作的强度。当机器停止工作，轻度工作和重度工作时$u_k^1$的值分别取0，1和2。假设机器的工作强度越高，它崩溃的可能性就越大。$u_k^2$表示维护机器所需的投入，假设它仅可以取值0或1，越大的值代表需要越多的维护。维护可以降低机器崩溃的概率，并使崩溃的机器恢复正常状态。

这两个控制的结果，可以用如下的控制转移概率建模。定义$u_k := (u_k^1, u_k^2)$，则
\begin{align}
P\{x_{k+1} = 2 \vert x_k = 1, x_{k-1},...,u_k,u_{k-1}...\} &= q_1(u_k^1) - q_2(u_k^2), \nonumber \\
P\{x_{k+1} = 1 \vert x_k = 1, x_{k-1},...,u_k,u_{k-1}...\} &= 1 - [q_1(u_k^1) - q_2(u_k^2)], \nonumber \\
P\{x_{k+1} = 1 \vert x_k = 2, x_{k-1},...,u_k,u_{k-1}...\} &= q_2(u_k^2), \nonumber \\
P\{x_{k+1} = 2 \vert x_k = 2, x_{k-1},...,u_k,u_{k-1}...\} &= 1 - q_2(u_k^2) \label{eq:4.1.3}
\end{align}
与公式\eqref{eq:4.1.1}类似，它们可以用一个矩阵表示，即控制$u$的函数：
\begin{equation}
P(u^1, u^2) = 
\begin{bmatrix}
1 - q_1(u^1) + q_2(u^2) & q_1(u^1) - q_2(u^2) \\
q_2(u^2) & 1 - q_2(u^2)
\end{bmatrix} \label{eq:4.1.4}
\end{equation}
公式\eqref{eq:4.1.3}可以用下面的状态图表示

\setlength{\unitlength}{1cm}
\thicklines
\begin{picture}(12,4)
\put(3.5,2){\circle{1}}
\put(3.4,1.9){$1$}
\put(8.5,2){\circle{1}}
\put(8.4,1.9){$2$}
\qbezier(3.9,2.4)(6,3.2)(8.1,2.4) % mid-upper curve
\put(7.9,2.45){\vector(4,-1){0.3}}
\put(5.6,3.3){$q_1 - q_2$}
\qbezier(3.9,1.6)(6,0.8)(8.1,1.6) % mid-lower curve
\put(4.1,1.55){\vector(-4,1){0.3}}
\put(5.9,0.5){$q_2$}
\qbezier(3.1,2.4)(1.5,2)(3.1,1.6) % left curve
\put(2.9,2.35){\vector(4,1){0.3}}
\put(0.2,1.9){$1 - [q_1 - q_2]$}
\qbezier(8.9,2.4)(10.5,2)(8.9,1.6) % right curve
\put(9.1,1.65){\vector(-4,-1){0.3}}
\put(10.5,1.9){$1 - q_2$}
\end{picture}

%==== End of Page 36 ====

其中，$q$的取值满足$q_1(0) < q_1(1) < q_1(2)$和$q_2(0) < q_2(1)$。这是因为轻度使用和维护得更好的机器比重度使用和维护不佳的机器崩溃的概率更低。另外，式\eqref{eq:4.1.3}中的最后两项概率与$u_k^1$无关，因为当机器已经崩溃时，它就无法被使用了。

假设系统受到一个时变的反馈指令$\{g_0,g_1,...\}$的作用，记$g_k \equiv g$，且另$u_k = g(x_k)$，则可得到转移概率矩阵$P_g = \{P_ij^g\}$，其中
\begin{equation}
P_ij^g := P_ij(g(i)), \quad i,j \in \{1,2\} \label{eq:4.1.5}
\end{equation}
例如，若$g(1) = (2,0), g(2) = (0,1)$，那么
\begin{equation}
P^g = 
\begin{bmatrix}
1 - q_1(2) + q_2(0) & q_1(2) - q_2(0) \\
q_2(1) & 1 - q_2(1)
\end{bmatrix} \label{eq:4.1.6}
\end{equation}

我们得到的结果$\{x_k\}$是一个具有静态转移概率$P^g$的马尔可夫链。$x_k$的概率分布可以写成行向量的形式
\begin{equation*}
p_k := (Prob\{x_k = 1\}, \ Prob\{x_k = 2\})
\end{equation*}
从马尔可夫的性质，公式\eqref{eq:4.1.3}可知
\begin{equation}
p_{k+m} = p_k[P^g]^m, \quad m \ge 0 \label{eq:4.1.7}
\end{equation}
特别地，
\begin{equation}
p_k = p_0[P^g]^k \label{eq:4.1.8}
\end{equation}
其中$p_0$是$x_0$的初始分布。

通常，当$k \to \infty$时，$p_k$收敛于与初始概率分布$p_0$无关的$p = (p(1),p(2))$，这时我们称它是一个具有各态历经性的马尔可夫链。概率分布的极限又称为稳态，平衡态，或不变分布。它是下面的线性方程的解：
\begin{equation}
p = p \ P^g, \quad p(1) + p(2) = 1 \label{eq:4.1.9}
\end{equation}
公式\eqref{eq:4.1.9}总有一个解。当马尔可夫链满足各态历经性时这个解是唯一的，且极限分布$p = (p(1),p(2))$可以用下式表示:
\begin{equation}
p(i) = \lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n I (x_k = i) \ wp \ 1 \label{eq:4.1.10}
\end{equation}
式中的$I$是指示函数，当$x_k = i$时$I(x_k = i) = 1$，当$x_k \ne i$时$I(x_k = i) = 0$。因此，$p(1)$是机器正常运转的时间的比例，$p(2)$是机器崩溃的时间的比例。

% ==== End of Page 37 ====

从式\eqref{eq:4.1.9}可以看出稳态概率$p$取决于反馈法则$g$。通过改变法则$g$，也就是改变使用和维护机器的时机，就可以调整机器崩溃的时间。但什么样的法则$g$可以导致最好的$p$呢？我们会在之后的章节进一步探讨这个问题。

\section{有限状态控制马尔可夫链}

前面的例子可以一般化成任意有限状态控制的马尔可夫链。它的状态$x_k$可取值$\{1,...I\}$。它的控制可以从一个有限或无限的集合$U$中取值。转移概率由一个$I \times I$的矩阵描述
\begin{equation}
u \to P(u) := \{P_ij(u), \quad 1 \le i,j \le I\} \label{eq:4.2.1}
\end{equation}
我们记
\begin{equation}
Prob\{x_{k+1} = j \vert x_k = i, x_{k-1},...,x_0,u_k,...,u_0\} = P_ij(u_k) \label{eq:4.2.2}
\end{equation}
矩阵$P(u)$的每个元素都是非负的，并且每一行的各个元素之和都是1，即矩阵$P(u)$是一个随机矩阵。同时，我们必须指定初始状态$x_0$的概率分布。当观测不等于状态，即$y_k \not\equiv x_k$，我们还需要指定观测概率
\begin{equation}
P(y \vert i) := Prob\{y_k = y \vert x_k = i\} \label{eq:4.2.3}
\end{equation}

\section{完全客观和马尔可夫策略}

考虑可控的马尔可夫链模型\eqref{eq:4.2.1}，若这个状态是可观测的，即$y_k \equiv x_k$，另$g = \{g_0,g_1,...\}$为一个反馈策略，使得$g_k$只与当前状态$x_k$有关。我们称这个策略$g$为马尔可夫策略。

若$g$是一个马尔可夫策略，且$\{x_k\}$是状态过程的结果，我们用一个$I$维的行向量表示$x_k$的概率分布：
\begin{equation*}
p_k^g := (Prob\{x_k = 1\},...,Prob\{x_k = I\})
\end{equation*}
上标$g$表示此分布由$g$决定，但在语义清楚时可以省略。

\begin{Lem} \label{Lem:4.3.1}
当我们运用了马尔可夫策略$g$的时候，状态过程的结果$\{x_k\}$就是一个马尔可夫过程。在$k$时刻它的一步转移概率为
% ==== End of Page 38 ====
\begin{equation*}
P_k^g := \{(P_k^g)_ij := P_ij (g_k(i)), \quad i \le i, j \le I\}
\end{equation*}
它的$m$步转移概率为
\begin{equation*}
P_k^g ... P_{k+m-1}^g
\end{equation*}
矩阵中第$ij$个元素是当$k$时刻它的状态为$i$的条件下$k+m$时刻它的状态为$j$的概率。即
\begin{equation*}
p_{k+m} = p_k \ P_k^g ... P_{k+m-1}^g
\end{equation*}
特别地，
\begin{equation}
p_k = p_0 \ P_0^g ... P_{k-1}^g \label{eq:4.3.2}
\end{equation}
其中$p_0$是初始状态$x_0$的概率分布。
\end{Lem}

\begin{Prf}
从马尔可夫的性质\eqref{eq:4.2.2}直接得证。
\end{Prf}

由于转移概率矩阵只取决于时间$k$，因此我们说$\{x_k\}$是一个具有非静态转移概率的马尔可夫链。

\section{马尔可夫策略的成本}

马尔可夫策略$g$决定了状态过程$\{x_k\}$的概率分布和控制过程$\{u_k = g_k(x_k)\}$。不同的策略会导致不同的概率分布。在最优控制中，我们希望找到最好的策略，这需要通过比较多种策略才能得出。我们为此定义了代价函数，它是一系列和状态与控制相关的实值函数
\begin{equation*}
c_k(i,u), \quad 1 \le i \le I, \quad u \in U, \quad k \ge 0
\end{equation*}
它表示$c_k{i,u}$是$k$时刻当$x_k = i$和$u_k = u$时的代价。

对于一个固定的马尔可夫策略$g$，到$N$时刻积累的代价为$\sum_{k=0}^N c_k(x_k,u_k)$。因为$x_k$和$u_k$是随机的，所以它是一个随机变量。积累的代价的均值为
\begin{equation}
J(g) := E^g \ \sum_{k=0}^N c_k(x_k,u_k) \ = \ E^g \ \sum_{k=0}^N c_k(x_k,g_k(x_k)) \label{eq:4.4.1}
\end{equation}
此处的$E^g$代表由$g$决定的$\{x_k\}$和$\{u_k\}$的概率分布。

$J(g)$可以通过转移概率矩阵$P_k^g$算出。从公式\eqref{eq:4.4.1}可以得到
\begin{align}
J(g) &= \sum_{k=0}^N \sum_{i=1}^I Prob\{x_k = i\} \ c_k(i,g_k(i)) \nonumber \\
% ==== End of Page 39 ====
&= \sum_{k=0}^N p_k c_k^g = \sum_{k=0}^N p_0 (P_0^g ... P_{k-1}^g) c_k^g \label{eq:4.4.2}
\end{align}
其中$c_k^g$是$I$维的列向量
\begin{equation}
c_k^g := (c_k(1,g(1)),...,c_k(I,g(I)))^T \label{eq:4.4.3}
\end{equation}
式\eqref{eq:4.4.2}中的最后一步推导用到了\eqref{eq:4.3.2}。

最佳的马尔可夫策略$g$可以使$J(g) = \sum_{k=0}^N p_0 (P_0^g ... P_{k-1}^g) c_k^g$取最小值。在第五章我们将通过学习动态规划来计算最佳的$g$。

动态规划是一个用递归方式计算最佳$g$的方法。由于这个方法只取决于下面的事实：由$g$决定的状态过程是马尔可夫的，因此我们在这里加以介绍。对任意时间$1 \le k \le N$以及状态$1 \le i \le I$，令$V_k^g(i)$代表当$x_k = i$时$k,...N$阶段代价的期望。也就是说
\begin{equation}
V_k^g(i) := E^g \ \{\sum_{l=k}^N c_l (x_l,g_l(x_l)) \vert x_k = i\} \label{eq:4.4.4}
\end{equation}
因此所有的代价可以表示为
\begin{equation}
J(g) = \sum_{i=1}^I (p_0)_i V_0^g (i) \label{eq:4.4.5}
\end{equation}

\begin{Lem} \label{Lem:4.4.6}
函数$V_k^g(i)$可以通过向后递归的方式计算：
\begin{equation}
V_k^g(i) = c_k(i,g_k(i)) + \sum_{j=1}^I (P_k^g)_ij V_{k+1}^g(j), \quad 0 \le k < N \label{eq:4.4.7}
\end{equation}
其初始条件是
\begin{equation}
V_N^g(i) = c_N(i,g_N(i)) \label{eq:4.4.8}
\end{equation}
\end{Lem}

\begin{Prf}
从定义中我们立即可得\eqref{eq:4.4.8}，之后
\begin{align*}
V_k^g(i) &= E^g \ \{\sum_{l=k}^N c_l(x_l,g_l(x_l)) \vert x_k = i\} \\
&= c_k(i,g_k(i)) + E^g \ \{E^g \ \{\sum_{l=k+1}^N c_l(x_l,g_l(x_l)) \vert x_{k+1},x_k = i\} \vert x_k = i\} \\
% ==== End of Page 40 ====
&= c_k(i,g_k(i)) + E^g \ \{V_{k+1}^g(x_{k+1}) \vert x_k = i\}, \quad by \eqref{eq:4.2.2} \\
&= c_k(ig_k(i)) + \sum_{j=1}^I V_{k+1}^g(j) Prob\{x_{k+1} \vert x_k = i\}
\end{align*}
由$(P_k^g)_{ij}$可看出，这就是\eqref{eq:4.4.7}的结果。
\end{Prf}

